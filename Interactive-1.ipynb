{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d8432-958f-4722-9128-5ac389ce00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load gaze estimator\n",
      "load the pre-trained model:  ./ckpt/epoch_24_ckpt.pth\n",
      "load input face image:  ./example/input\\cam00.JPG\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze0.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_09_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze1.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_19_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze2.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_21_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze3.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_22_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze4.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_23_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze5.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_25_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze6.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_26_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze7.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_27_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze8.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_33_Pro.jpg\n",
      "detected one face\n",
      "estimate head pose\n",
      "data normalization, i.e. crop the face image\n",
      "prepare the output\n",
      "save output image to:  example/output/results_gaze9.jpg\n",
      "load input face image:  ./example/input\\WIN_20251006_14_53_34_Pro.jpg\n",
      "warning: no detected face\n",
      "detected one face\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\amir.soltani\\Desktop\\DriverFatigueDetection\\ETH-XGaze\\demo.py:161\u001b[0m\n\u001b[0;32m    158\u001b[0m     exit(\u001b[39m0\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdetected one face\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m shape \u001b[39m=\u001b[39m predictor(\n\u001b[1;32m--> 161\u001b[0m     image, detected_faces[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    162\u001b[0m )  \u001b[39m## only use the first detected face (assume that each input image only contains one face)\u001b[39;00m\n\u001b[0;32m    163\u001b[0m shape \u001b[39m=\u001b[39m face_utils\u001b[39m.\u001b[39mshape_to_np(shape)\n\u001b[0;32m    164\u001b[0m landmarks \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mIndexError\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from model import gaze_network\n",
    "\n",
    "from head_pose import HeadPoseEstimator\n",
    "import glob\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        # this also convert pixel value from [0,255] to [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def estimateHeadPose(landmarks, face_model, camera, distortion, iterate=True):\n",
    "    ret, rvec, tvec = cv2.solvePnP(\n",
    "        face_model, landmarks, camera, distortion, flags=cv2.SOLVEPNP_EPNP\n",
    "    )\n",
    "\n",
    "    ## further optimize\n",
    "    if iterate:\n",
    "        ret, rvec, tvec = cv2.solvePnP(\n",
    "            face_model, landmarks, camera, distortion, rvec, tvec, True\n",
    "        )\n",
    "\n",
    "    return rvec, tvec\n",
    "\n",
    "\n",
    "def draw_gaze(image_in, pitchyaw, thickness=2, color=(0, 0, 255)):\n",
    "    \"\"\"Draw gaze angle on given image with a given eye positions.\"\"\"\n",
    "    image_out = image_in\n",
    "    (h, w) = image_in.shape[:2]\n",
    "    length = np.min([h, w]) / 2.0\n",
    "    pos = (int(w / 2.0), int(h / 2.0))\n",
    "    if len(image_out.shape) == 2 or image_out.shape[2] == 1:\n",
    "        image_out = cv2.cvtColor(image_out, cv2.COLOR_GRAY2BGR)\n",
    "    dx = -length * np.sin(pitchyaw[1]) * np.cos(pitchyaw[0])\n",
    "    dy = -length * np.sin(pitchyaw[0])\n",
    "    cv2.arrowedLine(\n",
    "        image_out,\n",
    "        tuple(np.round(pos).astype(np.int32)),\n",
    "        tuple(np.round([pos[0] + dx, pos[1] + dy]).astype(int)),\n",
    "        color,\n",
    "        thickness,\n",
    "        cv2.LINE_AA,\n",
    "        tipLength=0.2,\n",
    "    )\n",
    "\n",
    "    return image_out\n",
    "\n",
    "\n",
    "def normalizeData_face(img, face_model, landmarks, hr, ht, cam):\n",
    "    ## normalized camera parameters\n",
    "    focal_norm = 960  # focal length of normalized camera\n",
    "    distance_norm = 600  # normalized distance between eye and camera\n",
    "    roiSize = (224, 224)  # size of cropped eye image\n",
    "\n",
    "    ## compute estimated 3D positions of the landmarks\n",
    "    ht = ht.reshape((3, 1))\n",
    "    hR = cv2.Rodrigues(hr)[0]  # rotation matrix\n",
    "    Fc = np.dot(hR, face_model.T) + ht  # rotate and translate the face model\n",
    "    two_eye_center = np.mean(Fc[:, 0:4], axis=1).reshape((3, 1))\n",
    "    nose_center = np.mean(Fc[:, 4:6], axis=1).reshape((3, 1))\n",
    "    # get the face center\n",
    "    face_center = np.mean(\n",
    "        np.concatenate((two_eye_center, nose_center), axis=1), axis=1\n",
    "    ).reshape((3, 1))\n",
    "\n",
    "    ## ---------- normalize image ----------\n",
    "    distance = np.linalg.norm(\n",
    "        face_center\n",
    "    )  # actual distance between eye and original camera\n",
    "\n",
    "    z_scale = distance_norm / distance\n",
    "    cam_norm = np.array(\n",
    "        [  # camera intrinsic parameters of the virtual camera\n",
    "            [focal_norm, 0, roiSize[0] / 2],\n",
    "            [0, focal_norm, roiSize[1] / 2],\n",
    "            [0, 0, 1.0],\n",
    "        ]\n",
    "    )\n",
    "    S = np.array(\n",
    "        [  # scaling matrix\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "            [0.0, 0.0, z_scale],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hRx = hR[:, 0]\n",
    "    forward = (face_center / distance).reshape(3)\n",
    "    down = np.cross(forward, hRx)\n",
    "    down /= np.linalg.norm(down)\n",
    "    right = np.cross(down, forward)\n",
    "    right /= np.linalg.norm(right)\n",
    "    R = np.c_[right, down, forward].T  # rotation matrix R\n",
    "\n",
    "    W = np.dot(\n",
    "        np.dot(cam_norm, S), np.dot(R, np.linalg.inv(cam))\n",
    "    )  # transformation matrix\n",
    "\n",
    "    img_warped = cv2.warpPerspective(img, W, roiSize)  # warp the input image\n",
    "\n",
    "    # head pose after normalization\n",
    "    hR_norm = np.dot(R, hR)  # head pose rotation matrix in normalized space\n",
    "    hr_norm = cv2.Rodrigues(hR_norm)[0]  # convert rotation matrix to rotation vectors\n",
    "\n",
    "    # normalize the facial landmarks\n",
    "    num_point = landmarks.shape[0]\n",
    "    landmarks_warped = cv2.perspectiveTransform(landmarks, W)\n",
    "    landmarks_warped = landmarks_warped.reshape(num_point, 2)\n",
    "\n",
    "    return img_warped, landmarks_warped\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # img_file_name = './example/input/cam00.JPG'\n",
    "    img_file_names = glob.glob(\"./example/input/*.JPG\")\n",
    "    predictor = dlib.shape_predictor(\"./modules/shape_predictor_68_face_landmarks.dat\")\n",
    "    # face_detector = dlib.cnn_face_detection_model_v1('./modules/mmod_human_face_detector.dat')\n",
    "    face_detector = (\n",
    "        dlib.get_frontal_face_detector()\n",
    "    )  ## this face detector is not very powerful\n",
    "\n",
    "    print(\"load gaze estimator\")\n",
    "    model = gaze_network()\n",
    "    # model.cuda() # comment this line out if you are not using GPU\n",
    "    pre_trained_model_path = \"./ckpt/epoch_24_ckpt.pth\"\n",
    "    if not os.path.isfile(pre_trained_model_path):\n",
    "        print(\"the pre-trained gaze estimation model does not exist.\")\n",
    "        exit(0)\n",
    "    else:\n",
    "        print(\"load the pre-trained model: \", pre_trained_model_path)\n",
    "    ckpt = torch.load(pre_trained_model_path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(\n",
    "        ckpt[\"model_state\"], strict=True\n",
    "    )  # load the pre-trained model\n",
    "\n",
    "    i = 0\n",
    "    for img_file_name in img_file_names:\n",
    "        print(\"load input face image: \", img_file_name)\n",
    "\n",
    "        image = cv2.imread(img_file_name)\n",
    "\n",
    "        detected_faces = face_detector(\n",
    "            cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1\n",
    "        )  ## convert BGR image to RGB for dlib\n",
    "        if len(detected_faces) == 0:\n",
    "            print(\"warning: no detected face\")\n",
    "            exit(0)\n",
    "        print(\"detected one face\")\n",
    "        shape = predictor(\n",
    "            image, detected_faces[0]\n",
    "        )  ## only use the first detected face (assume that each input image only contains one face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        landmarks = []\n",
    "        for x, y in shape:\n",
    "            landmarks.append((x, y))\n",
    "        landmarks = np.asarray(landmarks)\n",
    "\n",
    "        # load camera information\n",
    "        cam_file_name = \"./example/input/cam00.xml\"  # this is camera calibration information file obtained with OpenCV\n",
    "        if not os.path.isfile(cam_file_name):\n",
    "            print(\"no camera calibration file is found.\")\n",
    "            exit(0)\n",
    "        fs = cv2.FileStorage(cam_file_name, cv2.FILE_STORAGE_READ)\n",
    "        camera_matrix = fs.getNode(\n",
    "            \"Camera_Matrix\"\n",
    "        ).mat()  # camera calibration information is used for data normalization\n",
    "        camera_distortion = fs.getNode(\"Distortion_Coefficients\").mat()\n",
    "\n",
    "        print(\"estimate head pose\")\n",
    "        # load face model\n",
    "        face_model_load = np.loadtxt(\n",
    "            \"face_model.txt\"\n",
    "        )  # Generic face model with 3D facial landmarks\n",
    "        landmark_use = [20, 23, 26, 29, 15, 19]  # we use eye corners and nose conners\n",
    "        face_model = face_model_load[landmark_use, :]\n",
    "        # estimate the head pose,\n",
    "        ## the complex way to get head pose information, eos library is required,  probably more accurrated\n",
    "        # landmarks = landmarks.reshape(-1, 2)\n",
    "        # head_pose_estimator = HeadPoseEstimator()\n",
    "        # hr, ht, o_l, o_r, _ = head_pose_estimator(image, landmarks, camera_matrix[cam_id])\n",
    "        ## the easy way to get head pose information, fast and simple\n",
    "        facePts = face_model.reshape(6, 1, 3)\n",
    "        landmarks_sub = landmarks[[36, 39, 42, 45, 31, 35], :]\n",
    "        landmarks_sub = landmarks_sub.astype(\n",
    "            float\n",
    "        )  # input to solvePnP function must be float type\n",
    "        landmarks_sub = landmarks_sub.reshape(\n",
    "            6, 1, 2\n",
    "        )  # input to solvePnP requires such shape\n",
    "        hr, ht = estimateHeadPose(\n",
    "            landmarks_sub, facePts, camera_matrix, camera_distortion\n",
    "        )\n",
    "\n",
    "        # data normalization method\n",
    "        print(\"data normalization, i.e. crop the face image\")\n",
    "        img_normalized, landmarks_normalized = normalizeData_face(\n",
    "            image, face_model, landmarks_sub, hr, ht, camera_matrix\n",
    "        )\n",
    "\n",
    "        model.eval()  # change it to the evaluation mode\n",
    "        input_var = img_normalized[:, :, [2, 1, 0]]  # from BGR to RGB\n",
    "        input_var = transformations(input_var)\n",
    "        input_var = torch.autograd.Variable(input_var.float())\n",
    "        input_var = input_var.view(\n",
    "            1, input_var.size(0), input_var.size(1), input_var.size(2)\n",
    "        )  # the input must be 4-dimension\n",
    "        pred_gaze = model(\n",
    "            input_var\n",
    "        )  # get the output gaze direction, this is 2D output as pitch and raw rotation\n",
    "        pred_gaze = pred_gaze[\n",
    "            0\n",
    "        ]  # here we assume there is only one face inside the image, then the first one is the prediction\n",
    "        pred_gaze_np = (\n",
    "            pred_gaze.cpu().data.numpy()\n",
    "        )  # convert the pytorch tensor to numpy array\n",
    "\n",
    "        print(\"prepare the output\")\n",
    "        # draw the facial landmarks\n",
    "        landmarks_normalized = landmarks_normalized.astype(\n",
    "            int\n",
    "        )  # landmarks after data normalization\n",
    "        for x, y in landmarks_normalized:\n",
    "            cv2.circle(img_normalized, (x, y), 5, (0, 255, 0), -1)\n",
    "        face_patch_gaze = draw_gaze(\n",
    "            img_normalized, pred_gaze_np\n",
    "        )  # draw gaze direction on the normalized face image\n",
    "        output_path = f\"example/output/results_gaze{i}.jpg\"\n",
    "        i += 1\n",
    "        print(\"save output image to: \", output_path)\n",
    "        cv2.imwrite(output_path, face_patch_gaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted Python 3.10.11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
